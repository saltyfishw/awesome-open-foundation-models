# 开源开放基础大模型


旨在记录开源开发大模型发展情况，随时更新，欢迎在**Issues**中提供提供**线索**和**素材**

使用数据请注明来源：**微信公众号：走向未来** 和 **仓库：https://github.com/wgwang/awesome-open-foundation-models**


Awesome family related to LLMS includes:
- https://github.com/wgwang/awesome-LLM-benchmarks
- https://github.com/wgwang/awesome-LLMs-In-China
- https://github.com/wgwang/awesome-open-foundation-models


大模型相关的Awesome系列包括：
- 大模型评测数据集：
  https://github.com/wgwang/awesome-LLM-benchmarks
- 中国大模型列表：
  https://github.com/wgwang/awesome-LLMs-In-China
- 开源开放基础大模型列表：
  https://github.com/wgwang/awesome-open-foundation-models


微信扫码关注我的微信公众号：**走向未来**，分享有关大模型、AGI、知识图谱、深度学习、强化学习、计算机视觉、自然语言处理等等与人工智能有关的内容。

![](imgs/走向未来.jpg)  

**Star一下，举手之劳！**

## 开源开放的基础大模型列表


|序号|名称|参数规模|数据规模|说明|
|:-|:-|:-|:-|:-|
|1|[LLaMA-2](Open-LLMs/llama2.md)|7B,13B,34B,70B|2T|可商用|
|2|[Falcon](Open-LLMs/falcon.md)|7B,40B,180B|3.5T|数据集[ RefinedWeb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb)|
|3|[baichuan-2](Open-LLMs/baichuan2.md)|7B,13B|2.6T|开放，商用需授权，[baichuan-1](Open-LLMs/baichuan.md)|
|4|[InternLM](Open-LLMs/internlm.md)|7B,20B|2.3T|开放，商用需授权|
|5|[BLOOM](Open-LLMs/bloom.md)|3B,7.1B,176B|366B|可商用，最为宽松，[详细介绍](https://mp.weixin.qq.com/s/ia-yrmXbnlooRA3K1hoTwQ)|
|6|GALACTICA|6.7B,30B,120B|106B|开放的科学文本和数据|
|7|[LLaMA](Open-LLMs/llama.md)|7B,13B,30B,65B|1.4T|Meta，代码开源，模型“泄露”,不可商用，[详细介绍](https://mp.weixin.qq.com/s/dKInMi6P80GXecUtR3WQsA)|
|8|MOSS-moon|16B|700B|6.67x1022 FLOPs|
|9|ChatGLM2|6B|1.4T||
|10|StableLM|3B,7B|800B||
|11|RedPajama-INCITE|3B,7B|1T||
|12|GPT-NeoX|20B|3.15M|800GB的[The Pile](https://arxiv.org/abs/2101.00027)数据集|
|13|OpenLLaMA|3B,7B,13B|1T||
|14|MPT|7B,30B|1T|
|15|Pythia|2.8B,6.9B,12B|300B||
|16|XGen|7B|1.5T||
|17|OPT|6.7B,13B,30B,66B,175B|180B||
|18|[Qwen](Open-LLMs/qwen.md)|7B,14B|2.4T,3.0T||
|19|XVERSE|13B,65B|1.4T,2.6T||
|20|[Aquila2](https://github.com/FlagAI-Open/Aquila2)|7B,34B|2T||
|21|Prithvi|||IBM+NASA,地理空间，100M（图片）|
|22|[Skywork](Open-LLMs/skywork.md)|13B|3.2T|昆仑万维·天工|
|23|[Deepseek Coder](https://github.com/deepseek-ai/DeepSeek-Coder)|1.3B,6.7B,33B|2T|Deepseek Coder comprises a series of code language models trained on both 87% code and 13% natural language in English and Chinese, with each model pre-trained on 2T tokens.|
|24|Aquila|7B||悟道·天鹰|
|25|Yi|6B,34B|3T||
|26|Mistral|7B||欧洲|
|27|Yuan-2|2B,51B,102B|||

## 非基础大模型
- WizardLM，WizardMath，WizardCoder
- Alpaca
- Vicuna
- Guanaco
- [CodeLLaMA](Open-LLMs/codellama.md)
  - 7B,13B,34B，基于LLaMA2，增加了650B左右的代码词元进行增量训练和微调



## 模型架构

- [GPTQ](https://github.com/IST-DASLab/gptq)
- [LLaMA](https://github.com/facebookresearch/llama)


