# 书生·浦语

摘要：上海人工智能实验室与商汤科技联合香港中文大学和复旦大学正式推出书生·浦语，目前开放了7B 和20B 两个参数规模的大模型。


## 基础信息

- 由上海人工智能实验室与商汤科技联合香港中文大学和复旦大学联合开发
- 发布于2023年9月20日(20B)
- 许可：商用需申请


## 资料集合

[代码仓库](https://github.com/InternLM/InternLM)


## 参数

|名称|InternLM-20B|InternLM-7B|
|:-|:-|:-|
|参数规模params|20B|7B|
|隐变量维度dimension|5120|4096|
|自注意力头的个数n heads|40|32|
|层数n layers|60|32|
|词表大小Vocab size|103168|103168|
|输入序列长度sequence length|4096|2048|
|数据规模词元数量n tokens|2.3T||
|训练时长Training GPU-hours(A100)|||

## 训练模型的算力设施

无披露

## 模型下载

- [InternLM-7B](https://huggingface.co/internlm/internlm-7b)
- [InternLM-7B-Chat](https://huggingface.co/internlm/internlm-chat-7b-v1_1)
- [InternLM-20B](https://huggingface.co/internlm/internlm-20b)
- [InternLM-20B-Chat](https://huggingface.co/internlm/internlm-20b-chat)
  


## 效果方面



## 训练数据

指2.3T Tokens的公开数据，未详细披露数据情况。




## 模型架构


  
  

